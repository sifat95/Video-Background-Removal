<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8"/>
    <link rel="stylesheet" type="text/css" href="styles.css"/>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/deeplab"></script>
    <script async src="opencv.js"></script>
  </head>

  <body>
    <video id="video" width="513" height="289" autoplay muted playsinline></video>
    <canvas id="canvas" width="513" height="289" style="display: none;"></canvas>
    <div id="background-container">
      <canvas id="canvasOut" width="513" height="289"></canvas>
    </div>
    <script>
      (async () => {
        const video = document.getElementById('video')
        const canvas = document.getElementById('canvas')
        const context = canvas.getContext('2d')
        
        const canvasOut = document.getElementById('canvasOut')
        const contextOut = canvasOut.getContext('2d')
      
        const modelName = 'pascal';   // set to your preferred model, either `pascal`, `cityscapes` or `ade20k`
        const quantizationBytes = 2;
        const model = await deeplab.load({base: modelName, quantizationBytes})
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: false,
          video: {
            facingMode: 'user'
          }
        })

        video.srcObject = stream
        
        predict()

        async function predict() {
          context.drawImage(video, 0, 0, 513, 289)

          const base = 'pascal'
          const rawSegmentationMap = await model.predict(canvas)
          const {legend, height, width, segmentationMap} = await model.segment(canvas)
          
          // const segmentationMap = deeplab.toSegmentationImage(deeplab.getColormap(base), deeplab.getLabels(base), rawSegmentationMap)
          
          var segmentationData = new ImageData(segmentationMap, width, height);
          contextOut.drawImage(canvas, 0, 0, canvas.width, canvas.height)
          var imageData = contextOut.getImageData(0,0, canvas.width, canvas.height)
          var pixel = imageData.data
          var data = segmentationData.data
          for (var p = 0; p<pixel.length; p+=4)
          {
            if (data[p] == 0) {
                pixel[p+3] = 0
            }
          }
          
          contextOut.globalAlpha = 1
          contextOut.imageSmoothingQuality = 'high'
          contextOut.imageSmoothingEnabled = true
          contextOut.putImageData(imageData, 0, 0)
          // contextOut.putImageData(myImageData, 0, 0);
          const c = document.getElementById('canvasOut')
          let image = cv.imread(c)
          let dilate = new cv.Mat()
          let erode = new cv.Mat()
          let src = new cv.Mat()
          let M = cv.Mat.ones(3, 3, cv.CV_8U)
          let anchor = new cv.Point(-1, -1)
          // cv.cvtColor(image, image, cv.COLOR_RGBA2GRAY)
          cv.erode(image, erode, M, anchor, 1, cv.BORDER_CONSTANT, cv.morphologyDefaultBorderValue())
          cv.dilate(image, dilate, M, anchor, 1, cv.BORDER_CONSTANT, cv.morphologyDefaultBorderValue())
          // cv.dilate(dilate, dilate, M, anchor, 1, cv.BORDER_CONSTANT, cv.morphologyDefaultBorderValue())
          let mask = new cv.Mat()
          let dtype = -1
          cv.subtract(dilate, erode, src, mask, dtype)

          cv.subtract(image, src, image, mask, dtype)

          // cv.threshold(image, image, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)
          cv.imshow(c, image)
          requestAnimationFrame(predict)
        }
        // capturer.save();

      })()

    </script>
</body>
 
</html>